# Tech Challenge - Fase 3

Este repositÃ³rio contÃ©m a soluÃ§Ã£o desenvolvida para o **Tech Challenge da Fase 3**, onde foi proposta a construÃ§Ã£o de uma pipeline de dados com machine learning, incluindo ingestÃ£o, processamento, treinamento e apresentaÃ§Ã£o dos resultados. A atividade integra conhecimentos adquiridos em fases anteriores.

## ðŸ“º ApresentaÃ§Ã£o 

Foi gravado um vÃ­deo explicativo demonstrando todas as etapas do projeto, desde a concepÃ§Ã£o atÃ© a entrega do modelo produtivo. Acesse o vÃ­deo pelo link:

ðŸ“¹ [Link para o vÃ­deo explicativo no YouTube](https://www.youtube.com/watch?v=7zqbxl3CFfU&ab_channel=Sebasti%C3%A3oFerreira)

## ðŸ§  VisÃ£o Geral do Projeto

O projeto foi idealizado a partir de um brainstorm em equipe, que resultou na definiÃ§Ã£o de uma arquitetura baseada em trÃªs DAGs utilizando Apache Airflow:

![Brainstorm de construÃ§Ã£o](img/brain-storm-structure.png)


1. **IngestÃ£o em tempo real (minuto a minuto):**
   - ResponsÃ¡vel por coletar dados continuamente e armazenÃ¡-los em um Data Lake (Amazon S3).

2. **Processamento por hora:**
   - Agrega e transforma os dados brutos para alimentar o modelo.
   - Utiliza Amazon Athena para leitura otimizada dos dados no S3.

3. **Re-treinamento do modelo:**
   - Periodicamente reprocessa os dados e re-treina o modelo com base nas novas informaÃ§Ãµes disponÃ­veis.

![Brainstorm de construÃ§Ã£o](img/mermaid_chart.png)

## ðŸ”§ Arquitetura TÃ©cnica

- **OrquestraÃ§Ã£o:** Apache Airflow (implantado via Docker Compose em uma EC2 na AWS)
- **Armazenamento:** Amazon S3 (Data Lake)
- **Consulta e processamento:** Amazon Athena
- **Modelo de Machine Learning:** Support Vector Classifier (SVC)
- **Ambiente de ExecuÃ§Ã£o:** Python, scikit-learn, pandas, seaborn, matplotlib
- **Controle de VersÃ£o e DocumentaÃ§Ã£o:** GitHub
- **ApresentaÃ§Ã£o:** VÃ­deo com storytelling (link abaixo)

![Desenho tecnico](img/desenho_tecnico.png)

## Escalabilidade e EvoluÃ§Ã£o da Arquitetura

Embora o treinamento de modelos maiores e mais complexos normalmente exija uma infraestrutura robusta, a arquitetura adotada neste projeto foi planejada para suportar a evoluÃ§Ã£o e atender aos requisitos de uso a longo prazo. Com o uso de Apache Airflow, Docker Compose, AWS S3 e Amazon Athena, implementamos uma soluÃ§Ã£o modular e escalÃ¡vel que pode ser adaptada conforme as necessidades futuras, sem comprometer a eficiÃªncia operacional.


## ðŸ³ Airflow com Docker Compose

Utilizamos o Docker Compose para orquestrar o ambiente do Airflow.

> ObservaÃ§Ã£o: Ajuste as variÃ¡veis de ambiente conforme necessÃ¡rio.

## ðŸ¤– Modelagem Preditiva

ApÃ³s diversas experimentaÃ§Ãµes, optamos pelo algoritmo Support Vector Classifier (SVC), que apresentou bom desempenho apÃ³s:

Oversampling: Para tratar o desbalanceamento entre as classes.

Feature Engineering: CriaÃ§Ã£o de novas features a partir das datas e extraÃ§Ã£o de informaÃ§Ãµes relevantes.

## ðŸ“¦ Estrutura do RepositÃ³rio

```sh
.
â”œâ”€â”€ airflow
â”‚   â”œâ”€â”€ dags
â”‚   â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â”‚   â”œâ”€â”€ dag_ingestao_weather.cpython-312.pyc
â”‚   â”‚   â”‚   â”œâ”€â”€ dag_model_retrain.cpython-312.pyc
â”‚   â”‚   â”‚   â”œâ”€â”€ dag_processamento_wheather.cpython-312.pyc
â”‚   â”‚   â”‚   â”œâ”€â”€ dag_weather_etl.cpython-312.pyc
â”‚   â”‚   â”‚   â””â”€â”€ test-dag.cpython-312.pyc
â”‚   â”‚   â”œâ”€â”€ dag_ingestao_weather.py
â”‚   â”‚   â”œâ”€â”€ dag_model_retrain.py
â”‚   â”‚   â”œâ”€â”€ dag_processamento_wheather.py
â”‚   â”‚   â””â”€â”€ include
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ __pycache__
â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.cpython-312.pyc
â”‚   â”‚       â”‚   â”œâ”€â”€ ml_utils.cpython-312.pyc
â”‚   â”‚       â”‚   â””â”€â”€ weather_utils.cpython-312.pyc
â”‚   â”‚       â””â”€â”€ ml_utils.py
â”‚   â”œâ”€â”€ docker-compose.yaml
â”‚   â”œâ”€â”€ dockerfile
â”‚   â”œâ”€â”€ logs
â”‚   â”œâ”€â”€ plugins
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ setup.sh
â”œâ”€â”€ img
â”‚   â”œâ”€â”€ mermaid_chart.png
â”‚   â”œâ”€â”€ brain-storm-structure.png
â”‚   â””â”€â”€ desenho tecnico.png
â”œâ”€â”€ infrastructure
â”‚   â”œâ”€â”€ athena.tf
â”‚   â”œâ”€â”€ backend.tf
â”‚   â”œâ”€â”€ crawler.tf
â”‚   â”œâ”€â”€ ec2.tf
â”‚   â”œâ”€â”€ iam.tf
â”‚   â”œâ”€â”€ permissions
â”‚   â”‚   â”œâ”€â”€ Policy_GlueCrawler.json
â”‚   â”‚   â”œâ”€â”€ Policy_Lambda_decompress_S3.json
â”‚   â”‚   â”œâ”€â”€ Role_GlueCrawler.json
â”‚   â”‚   â””â”€â”€ Role_Lambda_decompress_S3.json
â”‚   â”œâ”€â”€ provider.tf
â”‚   â”œâ”€â”€ storage.tf
â”‚   â””â”€â”€ variables.tf
â””â”€â”€ readme.md
```


## ðŸš€ Como Executar o Projeto

### PrÃ©-requisitos

- Docker e Docker Compose instalados.
- AWS CLI configurado com acesso ao S3 e Athena (caso deseje replicar o ambiente na AWS).

### Setup do Airflow com Docker Compose

1. Clone o repositÃ³rio.
2. Navegue atÃ© a pasta do projeto.
3. Execute o Docker Compose:

   ```bash
   docker-compose up -d
    ```

Ã‰ necessÃ¡rio configurar a aws_connection dentro do airflow exemplo:

![Desenho tecnico](img/connections_airflow.png)

Para esse projeto deixamos tambem configurada nas secrets do airflow a api_key, deixamos desincryptada para mostrar como ela fica no projeto:

![Desenho tecnico](img/api_key.png)

## ðŸ§ª AvaliaÃ§Ã£o e MÃ©tricas
O modelo foi avaliado com base em:

- AcurÃ¡cia

- F1-score

- Matriz de ConfusÃ£o

Os resultados indicaram boa capacidade de generalizaÃ§Ã£o, principalmente apÃ³s a aplicaÃ§Ã£o do oversampling e do feature engineering.




